\documentclass{segabs}
\usepackage{epsfig}
\usepackage{algorithm,algcompatible,amssymb,amsmath}
\usepackage{algpseudocode}
\usepackage{cprotect}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[11pt]{moresize}
\usepackage{anyfontsize}
%\usepackage{hyperref}
\usepackage[obeyspaces]{url}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{float}
\usepackage{verbatimbox}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{diagbox}
\usepackage{textcomp}
\usepackage{nicefrac}
\usepackage{bm}
\usepackage{array} % for "\newcolumntype" macro
\newcolumntype{C}{>{{}}c<{{}}}
\setlength\arraycolsep{0pt}
\usepackage{pythonhighlight}
\hypersetup{colorlinks,
citecolor=blue,
linkcolor=blue,
urlcolor=blue
}
% An example of defining macros
\newcommand{\rs}[1]{\mathstrut\mbox{\scriptsize\rm #1}}
\newcommand{\rr}[1]{\mbox{\rm #1}}


\begin{document}

\title{Elastic Reverse Time Migration using a novel wavelet-based lossy compressor.}
%Optimizing Elastic Reverse Time Migration with a Cutting-Edge Wavelet-Based Lossy Compressor

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\author{Oscar F. Mojica$^{1,2}$\footnote{1}, $^{1}$SENAI-CIMATEC Supercomputing Center and $^{2}$National Institute of Petroleum Geophysics (INCT-GP).}

\footer{}
\lefthead{}
\righthead{Exploring SPERR for error-bounded wavefield data compression}
\newcommand*\Laplace{\mathop{}\!\mathbin\bigtriangleup}

\maketitle

\begin{abstract}

Elastic Reverse Time Migration (RTM) is known to produce terabyte-scale datasets for a typical large-scale run. In response to the challenge of handling such massive data volumes, the adoption of error-bounded lossy compression techniques has emerged as a promising solution. These approaches offer a compelling avenue for reducing the storage footprint of scientific datasets while adhering to predefined thresholds for acceptable data distortion. As the landscape of lossy compression techniques continually evolves, rigorous testing within real-world applications such as elastic RTM becomes paramount. Therefore, in this study, we delve into the exploration of SPERR, a recent lossy compression framework leveraging wavelet transform. Our research seeks to provide a thorough analysis of this novel framework by contrasting it with two established lossy compressors, SZ3 and zfp. The goal is to evaluate SPERR's effectiveness in reducing the significant data volumes associated with elastic RTM.

%In the realm of seismic imaging techniques, the quest for efficient data compression methods has emerged as a pivotal research pursuit. In this context, the advent of lossy compression frameworks has sparked considerable interest, promising significant reductions in data storage requirements without compromising crucial scientific insights. Among these innovations, a recent development leveraging wavelet decomposition has garnered particular attention for its potential to revolutionize seismic data processing.

%This paper presents a comprehensive study aimed at evaluating the efficacy of a novel wavelet-based compression framework within the domain of Reverse Time Migration (RTM). RTM stands as a cornerstone technique in seismic imaging, facilitating the reconstruction of subsurface structures by simulating wave propagation in heterogeneous media. However, the sheer volume of data generated by RTM simulations poses formidable challenges in terms of storage and computational overhead.

%Our investigation centers on the comparative analysis of this cutting-edge wavelet-based compressor against two established compression methodologies commonly employed in RTM workflows. We delve into the intricacies of each compression technique, meticulously scrutinizing their performance across various metrics. Notably, our study underscores the remarkable compression ratios achieved by the wavelet-based approach, demonstrating its efficacy in significantly reducing data storage requirements.

%Furthermore, we delve into the nuanced trade-offs inherent in employing the wavelet-based compressor, particularly concerning computational costs. While our findings reveal a marginally higher computational overhead associated with this method compared to its counterparts, we argue that the benefits accrued in terms of optimized compression rates far outweigh these considerations.

%Through rigorous experimentation and analysis, this paper sheds light on the transformative potential of wavelet-based lossy compression in enhancing the efficiency and efficacy of RTM workflows. By elucidating the strengths and limitations of this innovative approach, we aim to provide valuable insights for researchers and practitioners alike, paving the way for enhanced seismic imaging methodologies in the pursuit of deeper geological understanding.

\end{abstract}

\section{Introduction}\label{sec:1}

RTM stands as a pivotal technique in seismic imaging analysis, often yielding extensive datasets necessitating storage for subsequent utilization during its runtime. The conventional approach involves transferring this vast data volume to peripheral devices and reloading it into memory as required, potentially imposing significant strain on I/O operations and storage capacity. In this context, the advent of lossy compression frameworks has sparked considerable interest, promising significant reductions in data storage requirements without compromising the final results.

%In recent years, researchers have turned their attention to addressing these challenges through innovative data compression techniques tailored specifically for RTM. The exploration of lossy compression methods offers promising avenues for mitigating the burden on storage and I/O systems while maintaining acceptable levels of data fidelity.

The work by \citet{dmitriev2022error} introduces the concept of error-bounded lossy compression in acoustic RTM, comprehensively evaluating state-of-the-art compressors such as zfp, SZ3, and Bitcomp. Their study highlights the importance of compression ratios and throughput metrics while analyzing the impact of compression errors on snapshot reconstruction and final stacked image quality.
Building upon this research, \citet{huang2023towards} present a novel hybrid lossy compression method named HyZ, specifically tailored for acoustic RTM applications. Their approach not only demonstrates significant improvements in overall RTM performance but also ensures satisfactory reconstructed data quality for post hoc analysis, making it a compelling solution for real-world seismic imaging challenges.
Furthermore, \citet{barbosa2023reverse} propose a dual strategy involving lossy and lossless wavefield compression for parallel multi-core and GPU-based acoustic RTM. By leveraging techniques such as OpenACC and zfp library integration, they successfully alleviate data transfer bottlenecks and reduce persistent storage requirements without compromising seismic image accuracy or quality. Lastly, \citet{kukreja2022lossy} introduce a pioneering method that combines checkpointing techniques with error-controlled lossy compression for Full-Waveform Inversion (FWI), another crucial aspect of geophysical exploration. Their approach aims to reduce data movement and memory footprint, thereby improving runtime efficiency and scalability in the era of Exascale computing. In this paper, we propose the integration of a wavelet transform-based lossy compressor into elastic RTM, inspired by the advancements showcased in the aforementioned studies. By harnessing the benefits of error-bounded compression techniques, we aim to enhance the efficiency, scalability, and practical applicability of elastic RTM in real-world scenarios.

%For large problems in seismic tomography, I/O operations and memory requirements become increasingly important. This is due to the fact that forward and adjoint wavefields have to be processed asynchronously in time. In particular, the forward wavefield has to accessed duringthe adjoint simulation for computing the gradient and, furthermore, both forward and adjoint wavefield are required to compute Hessian-vector products. Even for mid-size problems this can amount in several terabytes of data. It is well-known that memory requirements can be traded for additional simulations using checkpointing techniques[59]. However, this might increase the computing time significantly. A promising alternative is proposed in [41] where the wavefield is locally compressed using different floating point accuracies. It would be an interesting field for future research to tailor these methods to the computation of gradients or Hessian-vector products.

%[41] F. R. Dalmau, M Hanzich, J de la Puente, and N Gutierrez. “Lossy Data Compression with DCT Transforms”. In: EAGE Workshop on High Performance Computing for Upstream. 2014.

%[59] A. Griewank. “Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation”. In: Optimization Methods and Software 1.1 (1992), pp. 35–54.

% Efficient Inversion Methods for Constrained Parameter Identification in Full-Waveform Seismic Tomography

% --------------------------------------------------------------------------
\section{Error-bounded Lossy Compression}\label{sec:2}
% --------------------------------------------------------------------------

Lossy compression methods tailored for floating-point data offer substantial reductions in storage and transmission expenses while maintaining the necessary accuracy for analysis purposes. In the subsequent overview, we delineate two state-of-the-art lossy compressors, zfp and SZ3, alongside the relatively recent addition, SPERR. It's worth noting that the selection of these three compressors was strongly influenced by their status as open-source scientific data compressors. Each presents distinct design principles and implementation methodologies. SZ3 is a prediction-based lossy compressor, whereas ZFP and SPERR are transform-based lossy compressors.
\begin{itemize}
\item zfp \citep{lindstrom2014fixed} operates as a block-transform-based compressor, akin to the method used in JPEG compression. Its approach involves employing a custom orthogonal transform on data blocks, followed by encoding the transformed coefficients using specialized bitplane encoders. This unique methodology enables zfp to achieve high compression ratios and speeds, thanks to its optimized transform and encoding techniques. 

\item SPERR \citep{li2023lossy} is centered around wavelet transforms, which play a crucial role in its operation by yielding a set of coefficients that efficiently compact information. These coefficients encapsulate most of the input data's information within a small subset, their significance being directly proportional to their magnitude. Although wavelet transforms themselves do not inherently reduce data, SPERR introduces data reduction during the encoding of coefficients, albeit with the potential for information loss. To efficiently encode the addresses and values of these information-rich coefficients, SPERR leverages SPECK \citep{tang2006three}. After compressing input data with SPECK, SPERR identifies outliers by detecting reconstructed data points that exceed a specified PWE tolerance. It then records the positions of these outliers and calculates the correction values required to adjust them within the PWE tolerance, employing a SPECK-inspired algorithm. SPERR's compression method involves encoding both wavelet coefficients and outliers, generating distinct bitstreams for each stage. These bitstreams are subsequently merged and subjected to lossless compression using zstd \citep{collet2017zstandard}, resulting in the final SPERR output.
\item SZ3 \citep{zhao2021optimizing} operates on a prediction-based model. Its prediction module uses multi-dimensional dynamic spline interpolation. This advanced interpolation method eliminates the necessity of storing linear regression coefficients used in its predecessor (SZ2.1, \citet{liang2018error}), thus streamlining the compression process. Following prediction, SZ3 quantizes prediction errors and compresses resulting integers using Huffman encoding and zstd lossless compressor. These refinements contribute to improved compression ratios, particularly advantageous for scenarios with higher error bounds.

%SPERR is a compression method based on CDF 9/7 wavelets. Similar to SZ, it addresses outliers by encoding their corrections, but utilizes SPECK for this purpose. Wavelet transforms are central to SPERR's operation, resulting in a collection of coefficients that compact information efficiently. These coefficients retain most of the input's information in a small subset, proportional to their magnitude. While wavelet transforms themselves do not reduce data, during coefficient encoding, data reduction occurs, albeit with the possibility of introducing information loss. SPERR employs SPECK, an advanced algorithm, for efficient coding of the addresses and values of these information-rich coefficients. SPERR's compression process involves coding both wavelet coefficients and outliers, generating separate bitstreams for each step. These bitstreams are then combined and undergo lossless compression using ZSTD, producing the final SPERR output.
%SPERR utilizes CDF 9/7 wavelets for its operations, similar to SZ. It encodes outliers' corrections but with the utilization of SPECK. The core of SPERR lies in wavelet transforms. When applied to input data, wavelet transforms produce coefficients with high information compactness, where the majority of data resides in a small percentage of coefficients proportional to their magnitude. While wavelet transforms themselves don't reduce data, during coefficient coding, reduction and potential information loss occur. SPERR employs sophisticated algorithms, with SPECK being pivotal, to efficiently code addresses and values of these critical coefficients. SPERR's process involves coding wavelet coefficients and outliers separately, resulting in two bitstreams. These are then concatenated and undergo lossless compression via ZSTD, yielding the final SPERR output.
\end{itemize}


% --------------------------------------------------------------------------
\section{Elastic Reverse-Time Migration}\label{sec:3}
\subsection{Governing equations}
% --------------------------------------------------------------------------

In a heterogeneous elastic, anisotropic medium, the linear wave equation in 3-D Cartesian coordinates can be written as

\begin{equation}
\rho \partial _{tt} \bm{u}= \nabla \cdot \bm{\sigma} + \bm{f}
\end{equation}

with the constitutive relation

\begin{equation}
\bm{\sigma} = \bm{C: \epsilon}
\end{equation}

and the definition of the strain

\begin{equation}
\epsilon = \dfrac{1}{2}[\nabla \bm{u}  + (\nabla \bm{u})^{T}],
\end{equation}

where $\bm{u}$ is the displacement vector, $\bm{\sigma}$ denotes the stress tensor, $\bm{\epsilon}$ is the strain tensor; $\bm{C}$ is the fourth-order stiffness tensor of the elastic medium which is usually expressed by the Voigt notation $C_{ij} (i, j = 1, 2, \hdots , 6)$, $\rho$ is the solid mass density, and $\bm{f}$ stands for an external source force; the ``:'' symbol represents a double tensorial contraction operation, the superscript $T$ denotes the matrix transpose, and $\partial _{tt}$ is the second-order temporal derivative.

\subsection{Imaging condition}

From the several available options for elastic imaging conditions, we use a non-pure mode imaging condition (i.e., there is no mode separation in the wave simulation). This imaging condition cross-correlates the sum of normal stresses to obtain a PP image \citep{monsegny2019pure}:

\begin{equation}
I_{PP}= \int_0^T (\sigma_{xx} + \sigma_{yy} + \sigma_{zz})(\hat{\sigma}_{xx} + \hat{\sigma}_{yy} + \hat{\sigma}_{zz})dt,
\end{equation}

where $T$ is the maximum seismogram time and the hat (``\textasciicircum'') means reverse propagated. The 3D elastic RTM algorithm we used to perform the migrations is based on a 3D elastic finite difference modeling algorithm which is part of an in-house software package and is fourth-order accurate in time and eighth-order accurate in space. The elastic finite difference code is parallelized using OpenMP.

% --------------------------------------------------------------------------
\section{Numerical experiments}\label{sec:4}
% --------------------------------------------------------------------------
To study the use of lossy compressors for elastic RTM, we prepared a synthetic dataset using a subsection of the SEAM acoustic isotropic velocity model. We selected a specific segment representing a sedimentary basin, bordered by salt on one side. Additionally, we introduced an 800-meter-deep water layer with a constant velocity of 1500 m/s on top, simulating a moderately deep water environment. For the water layer, we set the s-wave velocity to zero, while in the sediments and salt, the s-wave velocity was uniformly defined as half the p-wave velocity divided by $2\sqrt{2}$. Both water and salt were assumed to be isotropic, with zero anisotropy parameters. In the sediment layers, we maintained constant anisotropic parameters: $\delta$ = 0.02, $\epsilon$ = 0.04, $\gamma$ = 0.01, azimuth ($\phi$) at 30°, and structural dip ($\theta$) at 10°. Density varied, with 1 g/cm${^{3}}$ in the water layer, 2 g/cm${^{3}}$ in all sediments, and 2.5 g/cm${^{3}}$ in the salt. We generated synthetic data using a P-wave point source with a Ricker wavelet peaking at 8 Hz, and a total recording time of 4 s. The dataset comprised 320 seismic shots. Upon acquiring the data, our evaluation proceeded using an industrial parallel elastic RTM code. This involved running the code through 4000 time steps, generating snapshots at each time step. Given our maximum offset constraint of 3 km, the physical parameters (vp, vs, density, and Thomsen parameters) were adjusted accordingly. With this adjustment, the forward propagation stage produced 4000 snapshots for compression, each containing 200x150x100 single-precision floating-point data points. Thus, the total demand to store the source wavefield without compression is 48 GB per shot.

We used commonly adopted metrics to evaluate scientific lossy compression, setting a point-wise absolute error of 1E-9 as the error-bound ($eb$) type. This $eb$ type is supported by all tested compressors, providing a basis for comparison, although not all of them guarantee the requested $eb$.

\begin{itemize}
\item Compression Ratio (CR): CR compares the original data size with compressed data size by
\begin{equation}
CR= \dfrac{\mbox{size(original data)}}{\mbox{size (compressed data)}}
\end{equation}

\item{Peak Signal-to-Noise Ratio (PSNR)}: measure the reconstructed data quality compared to the original data. It is defined as:
\begin{equation}
PSNR= 20\cdot log_{10}(\dfrac{max(x)-min(x)}{\sqrt{MSE(x,\hat{x})}}),
\end{equation}
where $MSE(x,\hat{x})$ represents the mean squared error between the original dataset $x$ and the decompressed dataset $\hat{x}$. A higher PSNR value signifies better data fidelity.
\end{itemize}

% --------------------------------------------------------------------------
\section{Conclusions}\label{sec:5}
% --------------------------------------------------------------------------


\section{Acknowledgements}

This work was supported by Repsol Sinopec and ANP through the SIREN project at Senai Cimatec. Oscar Mojica acknowledges the INCT-GP. The facility support from SENAI-CIMATEC Supercomputing Center is also acknowledged.

\onecolumn

\append{The source of the bibliography}
\verbatiminput{example.bib}

\twocolumn

\bibliographystyle{seg}  % style file is seg.bst
\bibliography{example}

\end{document}
